{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libs\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up default seeds\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device = gpu\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label_desc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>im having ssa examination tomorrow in the morn...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>i constantly worry about their fight against n...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>i feel its important to share this info for th...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>i truly feel that if you are passionate enough...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>i feel like i just wanna buy any cute make up ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence label_desc  label\n",
       "0                               i didnt feel humiliated    sadness      4\n",
       "1     i can go from feeling so hopeless to so damned...    sadness      4\n",
       "2      im grabbing a minute to post i feel greedy wrong      anger      0\n",
       "3     i am ever feeling nostalgic about the fireplac...       love      3\n",
       "4                                  i am feeling grouchy      anger      0\n",
       "...                                                 ...        ...    ...\n",
       "1995  im having ssa examination tomorrow in the morn...    sadness      4\n",
       "1996  i constantly worry about their fight against n...        joy      2\n",
       "1997  i feel its important to share this info for th...        joy      2\n",
       "1998  i truly feel that if you are passionate enough...        joy      2\n",
       "1999  i feel like i just wanna buy any cute make up ...        joy      2\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collecting all the data, encoding emotion labels\n",
    "df_test = pd.read_table('./emotion_dataset/test.txt', delimiter=';', names=['sentence','label'])\n",
    "df_train = pd.read_table('./emotion_dataset/train.txt', delimiter=';', names=['sentence','label'])\n",
    "df_val = pd.read_table('./emotion_dataset/val.txt', delimiter=';', names=['sentence','label'])\n",
    "df = pd.concat([df_train,df_test,df_val])\n",
    "labelencoder = LabelEncoder()\n",
    "df['label_enc'] = labelencoder.fit_transform(df['label'])\n",
    "df.rename(columns={'label':'label_desc','label_enc':'label'},inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_desc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>surprise</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label_desc  label\n",
       "0    sadness      4\n",
       "2      anger      0\n",
       "3       love      3\n",
       "6   surprise      5\n",
       "7       fear      1\n",
       "8        joy      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How do they encoded\n",
    "df[['label_desc','label']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         6761\n",
       "sadness     5797\n",
       "anger       2709\n",
       "fear        2373\n",
       "love        1641\n",
       "surprise     719\n",
       "Name: label_desc, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much samples for each emotion\n",
    "df.label_desc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\sorok\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Sentences list\n",
    "sentences = df.sentence.values\n",
    "\n",
    "# Importing BERT tokenizer, tokinizing sentences\n",
    "MAX_LEN = 128\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "input_ids = [tokenizer.encode(sent, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True) for sent in sentences]\n",
    "labels = df.label.values\n",
    "\n",
    "# Create an attention mask: 1 for all input tokens and 0 for all padding tokens\n",
    "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into a training set and a test set\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=SEED, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=SEED, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs).type(torch.LongTensor)\n",
    "validation_inputs = torch.tensor(validation_inputs).type(torch.LongTensor)\n",
    "train_labels = torch.tensor(train_labels).type(torch.LongTensor)\n",
    "validation_labels = torch.tensor(validation_labels).type(torch.LongTensor)\n",
    "train_masks = torch.tensor(train_masks).type(torch.LongTensor)\n",
    "validation_masks = torch.tensor(validation_masks).type(torch.LongTensor)\n",
    "\n",
    "# Select a batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader, its really heavy for memory\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sorok\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained BERT model with a single linear classification layer on top\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6).to(device)\n",
    "\n",
    "# Parameters\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-8\n",
    "epochs = 5\n",
    "num_warmup_steps = 0\n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=adam_epsilon, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8742ac22166f47f8b1ef2c198b1e3dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "Current Learning rate: 1.6000000000000003e-05\n",
      "Average Training loss: 0.32880924785985716\n",
      "Validation Accuracy: 0.94\n",
      "Validation MCC Accuracy: 0.9219738588209456 \n",
      "\n",
      "Epoch: 2/5\n",
      "Current Learning rate: 1.2e-05\n",
      "Average Training loss: 0.12358999280766067\n",
      "Validation Accuracy: 0.936\n",
      "Validation MCC Accuracy: 0.9151770955517914 \n",
      "\n",
      "Epoch: 3/5\n",
      "Current Learning rate: 8.000000000000001e-06\n",
      "Average Training loss: 0.09267675857121746\n",
      "Validation Accuracy: 0.9435\n",
      "Validation MCC Accuracy: 0.9270282879296582 \n",
      "\n",
      "Epoch: 4/5\n",
      "Current Learning rate: 4.000000000000001e-06\n",
      "Average Training loss: 0.07011680623573355\n",
      "Validation Accuracy: 0.94\n",
      "Validation MCC Accuracy: 0.9214847471024027 \n",
      "\n",
      "Epoch: 5/5\n",
      "Current Learning rate: 0.0\n",
      "Average Training loss: 0.048532570524011845\n",
      "Validation Accuracy: 0.938\n",
      "Validation MCC Accuracy: 0.9186534857797966 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loss and accuracy lists\n",
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "# Gradients gets accumulated by default\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch_n in trange(1, epochs+1, desc='Epoch'):\n",
    "  print(\"Epoch: \" + str(epoch_n) + \"/\" + str(epochs))\n",
    "\n",
    "  # Calculate total loss for this epoch\n",
    "  batch_loss = 0\n",
    "\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "    \n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    loss = outputs[0]\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip the norm of the gradients to 1.0\n",
    "    # Gradient clipping is not in AdamW anymore\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update learning rate schedule\n",
    "    scheduler.step()\n",
    "\n",
    "    # Clear the previous accumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Update tracking variables\n",
    "    batch_loss += loss.item()\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_train_loss = batch_loss / len(train_dataloader)\n",
    "\n",
    "  #store the current learning rate\n",
    "  for param_group in optimizer.param_groups:\n",
    "    print('Current Learning rate:', param_group['lr'])\n",
    "    learning_rate.append(param_group['lr'])\n",
    "    \n",
    "  train_loss_set.append(avg_train_loss)\n",
    "  print('Average Training loss:', avg_train_loss)\n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].to('cpu').numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    \n",
    "    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\n",
    "    \n",
    "    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n",
    "    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    eval_mcc_accuracy += tmp_eval_mcc_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print('Validation Accuracy:', eval_accuracy/nb_eval_steps)\n",
    "  print('Validation MCC Accuracy:', eval_mcc_accuracy/nb_eval_steps,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15861bc75e0>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model and tokenizer just in case\n",
    "model.save_pretrained('./mymodel/model/')\n",
    "tokenizer.save_pretrained('./mymodel/tokenizer/')\n",
    "\n",
    "# Most importantly, save trained model.state_dict\n",
    "torch.save(model.state_dict(), './mymodel/trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6).to(device)\n",
    "model.load_state_dict(torch.load('./mymodel/trained_model.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_1_clear</th>\n",
       "      <th>comment_2_clear</th>\n",
       "      <th>comment_3_clear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robinhood move goalpost didnt like score preve...</td>\n",
       "      <td>class action lawsuit filed government supposed...</td>\n",
       "      <td>basically robinhood stated account real benefi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please please please let happen world insane w...</td>\n",
       "      <td>u trademark law first use first file long u go...</td>\n",
       "      <td>sound silly year ago uk police tried challenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>penalty attempted suicide death</td>\n",
       "      <td>daily beating continue morale improves</td>\n",
       "      <td>understood said anyone look employment elsewhe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hey guy youre please shut u could use nice men...</td>\n",
       "      <td>750 million people europe even zucky wont give...</td>\n",
       "      <td>much agree politician say life would good with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ill take shit never thought would headline 800</td>\n",
       "      <td>tide pod challenge white house edition</td>\n",
       "      <td>got letter georgia congressman saying need hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>criterion bot use determine post</td>\n",
       "      <td>somewhat offtopic find interesting there resta...</td>\n",
       "      <td>taliban realise western civilisation isnt bad ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>dont know company actually enough market power...</td>\n",
       "      <td>appears using shortterm view inflate severity ...</td>\n",
       "      <td>fta merger merger jones said really gotten poi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>seems like pretty minor indictment even prosec...</td>\n",
       "      <td>look like legal document included report ill r...</td>\n",
       "      <td>im bot factual credibility grade selected pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>mod please let know need include source commen...</td>\n",
       "      <td>guardian articlehttpswwwtheguardiancomsport202...</td>\n",
       "      <td>interesting unsure science behind would settin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9901</th>\n",
       "      <td>coverage new york time httpswwwnytimescom20220...</td>\n",
       "      <td>im bot factual credibility grade selected pers...</td>\n",
       "      <td>valued apartment rented ivanka trump high 25 m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9902 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_1_clear  \\\n",
       "0     robinhood move goalpost didnt like score preve...   \n",
       "1     please please please let happen world insane w...   \n",
       "2                       penalty attempted suicide death   \n",
       "3     hey guy youre please shut u could use nice men...   \n",
       "4        ill take shit never thought would headline 800   \n",
       "...                                                 ...   \n",
       "9897                   criterion bot use determine post   \n",
       "9898  dont know company actually enough market power...   \n",
       "9899  seems like pretty minor indictment even prosec...   \n",
       "9900  mod please let know need include source commen...   \n",
       "9901  coverage new york time httpswwwnytimescom20220...   \n",
       "\n",
       "                                        comment_2_clear  \\\n",
       "0     class action lawsuit filed government supposed...   \n",
       "1     u trademark law first use first file long u go...   \n",
       "2                daily beating continue morale improves   \n",
       "3     750 million people europe even zucky wont give...   \n",
       "4                tide pod challenge white house edition   \n",
       "...                                                 ...   \n",
       "9897  somewhat offtopic find interesting there resta...   \n",
       "9898  appears using shortterm view inflate severity ...   \n",
       "9899  look like legal document included report ill r...   \n",
       "9900  guardian articlehttpswwwtheguardiancomsport202...   \n",
       "9901  im bot factual credibility grade selected pers...   \n",
       "\n",
       "                                        comment_3_clear  \n",
       "0     basically robinhood stated account real benefi...  \n",
       "1     sound silly year ago uk police tried challenge...  \n",
       "2     understood said anyone look employment elsewhe...  \n",
       "3     much agree politician say life would good with...  \n",
       "4     got letter georgia congressman saying need hol...  \n",
       "...                                                 ...  \n",
       "9897  taliban realise western civilisation isnt bad ...  \n",
       "9898  fta merger merger jones said really gotten poi...  \n",
       "9899  im bot factual credibility grade selected pers...  \n",
       "9900  interesting unsure science behind would settin...  \n",
       "9901  valued apartment rented ivanka trump high 25 m...  \n",
       "\n",
       "[9902 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load our data\n",
    "dtf = pd.read_json('data.json', lines=True)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text, stemm=False, lemm=True, stopwords=None):\n",
    "    # Remove punctuations, lowercase, strip\n",
    "    text = re.sub('_', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    \n",
    "    # Tokenize\n",
    "    texts = text.split()    \n",
    "    # Remove Stopwords\n",
    "    if stopwords is not None:\n",
    "        texts = [word for word in texts if word not in stopwords]\n",
    "                \n",
    "    # Stemming\n",
    "    if stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        texts = [ps.stem(word) for word in texts]\n",
    "                \n",
    "    # Lemmatisation\n",
    "    if lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        texts = [lem.lemmatize(word) for word in texts]\n",
    "            \n",
    "    # Collect lists\n",
    "    text = \" \".join(texts)\n",
    "    return text\n",
    "\n",
    "dtf['comment_1_clear'] = dtf[\"comment_1\"].apply(lambda x: preprocess_text(x, stemm=False, lemm=True, stopwords=stopwords))\n",
    "dtf['comment_2_clear'] = dtf[\"comment_2\"].apply(lambda x: preprocess_text(x, stemm=False, lemm=True, stopwords=stopwords))\n",
    "dtf['comment_3_clear'] = dtf[\"comment_3\"].apply(lambda x: preprocess_text(x, stemm=False, lemm=True, stopwords=stopwords))\n",
    "dtf = dtf[['comment_1_clear','comment_2_clear','comment_3_clear']]\n",
    "dtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sorok\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare our data to feed it to the model\n",
    "batch_size = 16\n",
    "MAX_LEN = 128\n",
    "\n",
    "# dtf.comment_1_clear.values is changed for comment_2_clear and comment_3_clear\n",
    "input_comms_1 = [tokenizer.encode(sent, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True) for sent in dtf.comment_1_clear.values]\n",
    "input_comms_1 = torch.tensor(input_comms_1).type(torch.LongTensor)\n",
    "att_masks_comms_1 = [[float(i>0) for i in seq] for seq in input_comms_1]\n",
    "att_masks_comms_1 = torch.tensor(att_masks_comms_1).type(torch.LongTensor)\n",
    "\n",
    "input_1 = TensorDataset(input_comms_1, att_masks_comms_1)\n",
    "input_1_dataloader = DataLoader(input_1, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeding data to our model to get predictions\n",
    "results = []\n",
    "for step, batch in enumerate(input_1_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    results.append(output.logits.softmax(dim=-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same done for c2,c3\n",
    "c1 = []\n",
    "for a in results:\n",
    "    for b in a:\n",
    "        c1.append(b.index(max(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int to label dict\n",
    "int_to_label = {\n",
    "  4: \"sadness\",\n",
    "  2: \"joy\",\n",
    "  0: \"anger\",\n",
    "  1: \"fear\",\n",
    "  5: \"surprise\",\n",
    "  3: \"love\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a result dataframe\n",
    "emotions_df = pd.DataFrame()\n",
    "emotions_df['enc_emote_c1'] = c1\n",
    "emotions_df['enc_emote_c2'] = c2\n",
    "emotions_df['enc_emote_c3'] = c3\n",
    "emotions_df['emote_c1'] = list(map(lambda x: int_to_label[x], c1))\n",
    "emotions_df['emote_c2'] = list(map(lambda x: int_to_label[x], c2))\n",
    "emotions_df['emote_c3'] = list(map(lambda x: int_to_label[x], c3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_emote_c1</th>\n",
       "      <th>enc_emote_c2</th>\n",
       "      <th>enc_emote_c3</th>\n",
       "      <th>emote_c1</th>\n",
       "      <th>emote_c2</th>\n",
       "      <th>emote_c3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>sadness</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>fear</td>\n",
       "      <td>joy</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>joy</td>\n",
       "      <td>anger</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>anger</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>anger</td>\n",
       "      <td>sadness</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>joy</td>\n",
       "      <td>anger</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>joy</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9901</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>anger</td>\n",
       "      <td>joy</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9902 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      enc_emote_c1  enc_emote_c2  enc_emote_c3 emote_c1 emote_c2  emote_c3\n",
       "0                0             0             0    anger    anger     anger\n",
       "1                0             4             0    anger  sadness     anger\n",
       "2                1             2             4     fear      joy   sadness\n",
       "3                2             0             2      joy    anger       joy\n",
       "4                0             0             0    anger    anger     anger\n",
       "...            ...           ...           ...      ...      ...       ...\n",
       "9897             0             2             2    anger      joy       joy\n",
       "9898             0             4             2    anger  sadness       joy\n",
       "9899             2             0             2      joy    anger       joy\n",
       "9900             2             0             0      joy    anger     anger\n",
       "9901             0             2             5    anger      joy  surprise\n",
       "\n",
       "[9902 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results\n",
    "emotions_df.to_csv('emotions_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71ff87c67ba816b6af4ee8e83b06c07dc669ccb76fd16d8077dc452b70bfd898"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
